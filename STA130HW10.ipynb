{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain succinctly in your own words (but working with a ChatBot if needed)...\n",
        "the difference between Simple Linear Regression and Multiple Linear Regression; and the benefit the latter provides over the former\n",
        "the difference between using a continuous variable and an indicator variable in Simple Linear Regression; and these two linear forms\n",
        "the change that happens in the behavior of the model (i.e., the expected nature of the data it models) when a single indicator variable is introduced alongside a continuous variable to create a Multiple Linear Regression; and these two linear forms (i.e., the Simple Linear Regression versus the Multiple Linear Regression)\n",
        "the effect of adding an interaction between a continuous and an indicator variable in Multiple Linear Regression models; and this linear form\n",
        "the behavior of a Multiple Linear Regression model (i.e., the expected nature of the data it models) based only on indicator variables derived from a non-binary categorical variable; this linear form; and the necessarily resulting binary variable encodings it utilizes\n"
      ],
      "metadata": {
        "id": "OGIfxYfhXLFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Distinction between Simple Linear Regression (SLR) and Multiple Linear Regression (MLR)**:\n",
        "   - **SLR** examines the relationship between a single independent variable (predictor) and a dependent variable (response), representing it as a straight line. It assumes a simple, direct connection where changes in the predictor directly affect the response.\n",
        "   - **MLR** uses more than one predictor variable, enabling it to capture more complex relationships. This makes MLR more accurate and realistic, as it can account for multiple factors at once.\n",
        "\n",
        "2. **Difference between using a continuous variable and an indicator variable in SLR**:\n",
        "   - A **continuous variable** can take any value within a range and directly affects the response proportionally in SLR.\n",
        "   - An **indicator variable** (or dummy variable) represents categories with binary values (0 or 1), allowing the model to adjust the intercept based on category, which is useful for handling categorical data.\n",
        "\n",
        "3. **Adding an indicator variable alongside a continuous variable in MLR**:\n",
        "   - Including both an indicator and a continuous variable in MLR enables the model to estimate the effect of the continuous variable while also adjusting for differences across categories represented by the indicator. This lets the model reflect changes in the regression line for each category, allowing interactions between categorical and continuous data.\n",
        "\n",
        "4. **Adding an interaction between a continuous and an indicator variable in MLR**:\n",
        "   - Introducing an interaction term between a continuous and an indicator variable in MLR allows the slope of the regression line to vary by category. This means the influence of the continuous variable on the outcome can differ based on the category, offering a more detailed view of how different groups respond to the same variable.\n",
        "\n",
        "5. **Behavior of an MLR model based only on indicator variables from a non-binary categorical variable**:\n",
        "   - In MLR, if the model is built only on indicator variables derived from a multi-category (non-binary) variable, each category (except a reference category) gets its own indicator variable. This approach enables the model to calculate different intercepts for each category, capturing the unique impact of each category on the response. One-hot encoding is typically used to convert categorical variables into multiple binary variables, allowing for separate analysis of each category’s effect.\n"
      ],
      "metadata": {
        "id": "HsfewvWtXMCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain in your own words (but working with a ChatBot if needed) what the specific (outcome and predictor) variables are for the scenario below; whether or not any meaningful interactions might need to be taken into account when predicting the outcome; and provide the linear forms with and without the potential interactions that might need to be considered\n",
        "Imagine a company that sells sports equipment. The company runs advertising campaigns on TV and online platforms. The effectiveness of the TV ad might depend on the amount spent on online advertising and vice versa, leading to an interaction effect between the two advertising mediums.\n",
        "\n",
        "Explain how to use these two formulas to make predictions of the outcome, and give a high level explaination in general terms of the difference between predictions from the models with and without the interaction\n",
        "\n",
        "Explain how to update and use the implied two formulas to make predictions of the outcome if, rather than considering two continuous predictor variables, we instead suppose the advertisement budgets are simply categorized as either \"high\" or \"low\" (binary variables)"
      ],
      "metadata": {
        "id": "HFp053t3XW-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identifying Variables and Potential Interactions\n",
        "\n",
        "1. **Outcome Variable**:\n",
        "   - The outcome variable here is likely **sales** (or **revenue**), as the company would be interested in understanding how their advertising budget influences the amount of sports equipment sold.\n",
        "\n",
        "2. **Predictor Variables**:\n",
        "   - The predictor variables are the **TV advertising budget** and the **online advertising budget**. These variables represent the amount of money spent on each platform.\n",
        "\n",
        "3. **Interaction Effect**:\n",
        "   - An interaction effect may exist between the **TV advertising budget** and **online advertising budget**. This means that the effectiveness of one type of advertising (e.g., TV) might vary depending on the amount spent on the other type (e.g., online), and vice versa.\n",
        "   - For example, if spending on both TV and online advertising is high, they might reinforce each other, leading to a greater impact on sales than if only one of them were high. Conversely, if spending on one medium is low, it might reduce the effectiveness of the other medium.\n",
        "\n",
        "### Linear Models\n",
        "\n",
        "1. **Model Without Interaction**:\n",
        "   - Without accounting for interaction, the linear model would look like this:\n",
        "     \\[\n",
        "     \\text{Sales} = \\beta_0 + \\beta_1 \\times \\text{TV\\_Budget} + \\beta_2 \\times \\text{Online\\_Budget}\n",
        "     \\]\n",
        "   - In this model, each advertising budget independently affects sales. The influence of the TV budget on sales is captured by \\(\\beta_1\\), and the influence of the online budget on sales is captured by \\(\\beta_2\\). There is no adjustment for the way these budgets might interact.\n",
        "\n",
        "2. **Model With Interaction**:\n",
        "   - With the interaction term, the model includes a combined effect of the TV and online budgets:\n",
        "     \\[\n",
        "     \\text{Sales} = \\beta_0 + \\beta_1 \\times \\text{TV\\_Budget} + \\beta_2 \\times \\text{Online\\_Budget} + \\beta_3 \\times (\\text{TV\\_Budget} \\times \\text{Online\\_Budget})\n",
        "     \\]\n",
        "   - Here, \\(\\beta_3\\) represents the interaction effect between the two budgets. This term adjusts the model’s prediction of sales based on the combined influence of both advertising budgets, meaning the effect of one budget can depend on the level of the other budget.\n",
        "\n",
        "### Making Predictions and Comparing Models\n",
        "\n",
        "1. **Predictions Without Interaction**:\n",
        "   - In the model without interaction, the predicted sales are simply a sum of the effects from TV and online budgets. This model assumes that each advertising budget impacts sales independently, with no consideration of their combined effect.\n",
        "   - For example, if TV advertising has a positive effect on sales, this will always be the case, regardless of the online advertising budget.\n",
        "\n",
        "2. **Predictions With Interaction**:\n",
        "   - In the model with interaction, the predicted sales take into account how the TV and online budgets work together. The presence of \\(\\beta_3\\) allows for the possibility that increasing one budget could amplify or diminish the effect of the other.\n",
        "   - For example, if both budgets are high, the interaction term might lead to a greater-than-expected increase in sales. Conversely, if one budget is low, it might reduce the overall effectiveness, which the model can now capture.\n",
        "\n",
        "### Adapting to Binary Predictor Variables\n",
        "\n",
        "If instead of continuous budgets, we classify the TV and online budgets as either \"high\" or \"low\" (binary variables), we need to adjust the model:\n",
        "\n",
        "1. **Without Interaction (Binary Predictors)**:\n",
        "   - Each predictor (TV and online budget) can now take values of 0 (low) or 1 (high). The model becomes:\n",
        "     \\[\n",
        "     \\text{Sales} = \\beta_0 + \\beta_1 \\times \\text{TV\\_High} + \\beta_2 \\times \\text{Online\\_High}\n",
        "     \\]\n",
        "   - Here, \\(\\text{TV\\_High}\\) and \\(\\text{Online\\_High}\\) are binary indicators for high spending on TV and online advertising, respectively.\n",
        "\n",
        "2. **With Interaction (Binary Predictors)**:\n",
        "   - The interaction term is the product of the binary variables:\n",
        "     \\[\n",
        "     \\text{Sales} = \\beta_0 + \\beta_1 \\times \\text{TV\\_High} + \\beta_2 \\times \\text{Online\\_High} + \\beta_3 \\times (\\text{TV\\_High} \\times \\text{Online\\_High})\n",
        "     \\]\n",
        "   - In this model, \\(\\beta_3\\) captures the combined effect of high TV and high online budgets. When both are set to high, the interaction term contributes to the prediction, potentially increasing sales more than the sum of the individual effects.\n",
        "\n",
        "### General Explanation of Model Differences\n",
        "\n",
        "- **Without Interaction**: Each variable (TV or online budget) independently influences sales, meaning the effect of TV is the same regardless of the online budget and vice versa.\n",
        "- **With Interaction**: The effect of each budget depends on the level of the other. This model captures a more nuanced relationship, as it allows for cases where high spending on both platforms may have an enhanced effect on sales, or where low spending on one platform may diminish the effectiveness of the other.\n",
        "\n",
        "The interaction model is generally more flexible and can capture complex relationships, whereas the non-interaction model assumes additive effects without cross-dependency between predictors."
      ],
      "metadata": {
        "id": "9YXLurHxTfuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Use smf to fit multiple linear regression models to the course project dataset from the canadian social connection survey"
      ],
      "metadata": {
        "id": "bdNAnDWYhs1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_proj=smf.ols(formula='LONELY_ucla_loneliness_scale_score (cohort 2022)~LONELY_dejong_emotional_social_loneliness_scale_score (cohort 2022)+PSYCH_zimet_multidimensional_social_support_scale_score (cohort 2022)',data=dataV2_cohortV4_wideV2)"
      ],
      "metadata": {
        "id": "8XlTWfY2ANXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# Load the dataset (replace with the correct URL for the Canadian Social Connection Survey)\n",
        "url = \"https://raw.githubusercontent.com/pointOfive/stat130chat130/main/CP/STA130F24_CourseProject_data.csv\"\n",
        "data = pd.read_csv(url).fillna('None')  # Fill missing values if needed\n",
        "\n",
        "# Define a binary variable for an example purpose (e.g., whether 'Gender' is Male)\n",
        "data['is_male'] = (data['Gender'] == 'Male').astype(int)\n",
        "\n",
        "# Specify the linear model formula with interactions and categorical variables\n",
        "# Modify this formula to include the actual variables of interest in your dataset.\n",
        "linear_model_formula = 'is_male ~ Age * Education_Level + Income * Marital_Status + C(Region)'\n",
        "\n",
        "# Fit the model\n",
        "linear_model_fit = smf.ols(formula=linear_model_formula, data=data).fit()\n",
        "\n",
        "# Display the summary of the model\n",
        "print(linear_model_fit.summary())"
      ],
      "metadata": {
        "id": "81C3qIWHUZc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of binary categorization\n",
        "data['high_income'] = (data['Income'] > threshold).astype(int)  # Replace `threshold` with an appropriate value\n",
        "data['high_age'] = (data['Age'] > threshold_age).astype(int)    # Similarly for Age\n",
        "\n",
        "# Update formula to include binary predictors and interactions\n",
        "binary_model_formula = 'is_male ~ high_age * high_income + C(Education_Level) + C(Marital_Status)'\n",
        "\n",
        "# Fit the model with binary predictors\n",
        "binary_model_fit = smf.ols(formula=binary_model_formula, data=data).fit()\n",
        "\n",
        "# Display the summary\n",
        "print(binary_model_fit.summary())"
      ],
      "metadata": {
        "id": "nbdN1nawUfI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Explain the apparent contradiction between the factual statements regarding the fit below that \"the model only explains 17.6% of the variability in the data\" while at the same time \"many of the coefficients are larger than 10 while having strong or very strong evidence against the null hypothesis of 'no effect'\""
      ],
      "metadata": {
        "id": "e566pOG4htMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
        "# fail https://github.com/KeithGalli/pandas/blob/master/pokemon_data.csv\n",
        "pokeaman = pd.read_csv(url)\n",
        "pokeaman\n",
        "\n",
        "\n",
        "model1_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation)', data=pokeaman) # MLR\n",
        "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation) + Q(\"Sp. Def\"):C(Generation)', data=pokeaman)#MLR(Q(\"Sp. Def\"):C(Generation)' intersaction)\n",
        "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") * C(Generation)', data=pokeaman) # MLR only with intersaction\n",
        "\n",
        "model2_fit = model2_spec.fit()\n",
        "model2_fit.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "1VPBRc_4huAp",
        "outputId": "0ee8039a-460a-4870-bd47-1fba02419841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                            OLS Regression Results                            \n",
              "==============================================================================\n",
              "Dep. Variable:                     HP   R-squared:                       0.176\n",
              "Model:                            OLS   Adj. R-squared:                  0.164\n",
              "Method:                 Least Squares   F-statistic:                     15.27\n",
              "Date:                Thu, 07 Nov 2024   Prob (F-statistic):           3.50e-27\n",
              "Time:                        02:50:00   Log-Likelihood:                -3649.4\n",
              "No. Observations:                 800   AIC:                             7323.\n",
              "Df Residuals:                     788   BIC:                             7379.\n",
              "Df Model:                          11                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "===================================================================================================\n",
              "                                      coef    std err          t      P>|t|      [0.025      0.975]\n",
              "---------------------------------------------------------------------------------------------------\n",
              "Intercept                          26.8971      5.246      5.127      0.000      16.599      37.195\n",
              "C(Generation)[T.2]                 20.0449      7.821      2.563      0.011       4.692      35.398\n",
              "C(Generation)[T.3]                 21.3662      6.998      3.053      0.002       7.629      35.103\n",
              "C(Generation)[T.4]                 31.9575      8.235      3.881      0.000      15.793      48.122\n",
              "C(Generation)[T.5]                  9.4926      7.883      1.204      0.229      -5.982      24.968\n",
              "C(Generation)[T.6]                 22.2693      8.709      2.557      0.011       5.173      39.366\n",
              "Q(\"Sp. Def\")                        0.5634      0.071      7.906      0.000       0.423       0.703\n",
              "Q(\"Sp. Def\"):C(Generation)[T.2]    -0.2350      0.101     -2.316      0.021      -0.434      -0.036\n",
              "Q(\"Sp. Def\"):C(Generation)[T.3]    -0.3067      0.093     -3.300      0.001      -0.489      -0.124\n",
              "Q(\"Sp. Def\"):C(Generation)[T.4]    -0.3790      0.105     -3.600      0.000      -0.586      -0.172\n",
              "Q(\"Sp. Def\"):C(Generation)[T.5]    -0.0484      0.108     -0.447      0.655      -0.261       0.164\n",
              "Q(\"Sp. Def\"):C(Generation)[T.6]    -0.3083      0.112     -2.756      0.006      -0.528      -0.089\n",
              "==============================================================================\n",
              "Omnibus:                      337.229   Durbin-Watson:                   1.505\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2871.522\n",
              "Skew:                           1.684   Prob(JB):                         0.00\n",
              "Kurtosis:                      11.649   Cond. No.                     1.40e+03\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "[2] The condition number is large, 1.4e+03. This might indicate that there are\n",
              "strong multicollinearity or other numerical problems.\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.176</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.164</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.27</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Thu, 07 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>3.50e-27</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>02:50:00</td>     <th>  Log-Likelihood:    </th> <td> -3649.4</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td>   800</td>      <th>  AIC:               </th> <td>   7323.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td>   788</td>      <th>  BIC:               </th> <td>   7379.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "                 <td></td>                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th>                       <td>   26.8971</td> <td>    5.246</td> <td>    5.127</td> <td> 0.000</td> <td>   16.599</td> <td>   37.195</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(Generation)[T.2]</th>              <td>   20.0449</td> <td>    7.821</td> <td>    2.563</td> <td> 0.011</td> <td>    4.692</td> <td>   35.398</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(Generation)[T.3]</th>              <td>   21.3662</td> <td>    6.998</td> <td>    3.053</td> <td> 0.002</td> <td>    7.629</td> <td>   35.103</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(Generation)[T.4]</th>              <td>   31.9575</td> <td>    8.235</td> <td>    3.881</td> <td> 0.000</td> <td>   15.793</td> <td>   48.122</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(Generation)[T.5]</th>              <td>    9.4926</td> <td>    7.883</td> <td>    1.204</td> <td> 0.229</td> <td>   -5.982</td> <td>   24.968</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(Generation)[T.6]</th>              <td>   22.2693</td> <td>    8.709</td> <td>    2.557</td> <td> 0.011</td> <td>    5.173</td> <td>   39.366</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Q(\"Sp. Def\")</th>                    <td>    0.5634</td> <td>    0.071</td> <td>    7.906</td> <td> 0.000</td> <td>    0.423</td> <td>    0.703</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Q(\"Sp. Def\"):C(Generation)[T.2]</th> <td>   -0.2350</td> <td>    0.101</td> <td>   -2.316</td> <td> 0.021</td> <td>   -0.434</td> <td>   -0.036</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Q(\"Sp. Def\"):C(Generation)[T.3]</th> <td>   -0.3067</td> <td>    0.093</td> <td>   -3.300</td> <td> 0.001</td> <td>   -0.489</td> <td>   -0.124</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Q(\"Sp. Def\"):C(Generation)[T.4]</th> <td>   -0.3790</td> <td>    0.105</td> <td>   -3.600</td> <td> 0.000</td> <td>   -0.586</td> <td>   -0.172</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Q(\"Sp. Def\"):C(Generation)[T.5]</th> <td>   -0.0484</td> <td>    0.108</td> <td>   -0.447</td> <td> 0.655</td> <td>   -0.261</td> <td>    0.164</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Q(\"Sp. Def\"):C(Generation)[T.6]</th> <td>   -0.3083</td> <td>    0.112</td> <td>   -2.756</td> <td> 0.006</td> <td>   -0.528</td> <td>   -0.089</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>337.229</td> <th>  Durbin-Watson:     </th> <td>   1.505</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2871.522</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>          <td> 1.684</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>      <td>11.649</td>  <th>  Cond. No.          </th> <td>1.40e+03</td>\n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}                  &        HP        & \\textbf{  R-squared:         } &     0.176   \\\\\n\\textbf{Model:}                          &       OLS        & \\textbf{  Adj. R-squared:    } &     0.164   \\\\\n\\textbf{Method:}                         &  Least Squares   & \\textbf{  F-statistic:       } &     15.27   \\\\\n\\textbf{Date:}                           & Thu, 07 Nov 2024 & \\textbf{  Prob (F-statistic):} &  3.50e-27   \\\\\n\\textbf{Time:}                           &     02:50:00     & \\textbf{  Log-Likelihood:    } &   -3649.4   \\\\\n\\textbf{No. Observations:}               &         800      & \\textbf{  AIC:               } &     7323.   \\\\\n\\textbf{Df Residuals:}                   &         788      & \\textbf{  BIC:               } &     7379.   \\\\\n\\textbf{Df Model:}                       &          11      & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}                &    nonrobust     & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                                         & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{Intercept}                       &      26.8971  &        5.246     &     5.127  &         0.000        &       16.599    &       37.195     \\\\\n\\textbf{C(Generation)[T.2]}              &      20.0449  &        7.821     &     2.563  &         0.011        &        4.692    &       35.398     \\\\\n\\textbf{C(Generation)[T.3]}              &      21.3662  &        6.998     &     3.053  &         0.002        &        7.629    &       35.103     \\\\\n\\textbf{C(Generation)[T.4]}              &      31.9575  &        8.235     &     3.881  &         0.000        &       15.793    &       48.122     \\\\\n\\textbf{C(Generation)[T.5]}              &       9.4926  &        7.883     &     1.204  &         0.229        &       -5.982    &       24.968     \\\\\n\\textbf{C(Generation)[T.6]}              &      22.2693  &        8.709     &     2.557  &         0.011        &        5.173    &       39.366     \\\\\n\\textbf{Q(\"Sp. Def\")}                    &       0.5634  &        0.071     &     7.906  &         0.000        &        0.423    &        0.703     \\\\\n\\textbf{Q(\"Sp. Def\"):C(Generation)[T.2]} &      -0.2350  &        0.101     &    -2.316  &         0.021        &       -0.434    &       -0.036     \\\\\n\\textbf{Q(\"Sp. Def\"):C(Generation)[T.3]} &      -0.3067  &        0.093     &    -3.300  &         0.001        &       -0.489    &       -0.124     \\\\\n\\textbf{Q(\"Sp. Def\"):C(Generation)[T.4]} &      -0.3790  &        0.105     &    -3.600  &         0.000        &       -0.586    &       -0.172     \\\\\n\\textbf{Q(\"Sp. Def\"):C(Generation)[T.5]} &      -0.0484  &        0.108     &    -0.447  &         0.655        &       -0.261    &        0.164     \\\\\n\\textbf{Q(\"Sp. Def\"):C(Generation)[T.6]} &      -0.3083  &        0.112     &    -2.756  &         0.006        &       -0.528    &       -0.089     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       & 337.229 & \\textbf{  Durbin-Watson:     } &    1.505  \\\\\n\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2871.522  \\\\\n\\textbf{Skew:}          &   1.684 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n\\textbf{Kurtosis:}      &  11.649 & \\textbf{  Cond. No.          } & 1.40e+03  \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n [2] The condition number is large, 1.4e+03. This might indicate that there are \\newline\n strong multicollinearity or other numerical problems."
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Low \\( R^2 \\) Value (17.6%): This suggests that, even if some individual predictors may be statistically significant, the model as a whole does not account for much of the variation in the dependent variable (HP). An \\( R^2 \\) value of 17.6% means that the model explains only about 17.6% of the variability in HP around its average. This implies that other factors not included in the model likely have a substantial impact on HP, or that the relationship between the predictors and HP is naturally complex or contains a lot of random noise."
      ],
      "metadata": {
        "id": "MRQpWc3cXcla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Discuss the following (five cells of) code and results with a ChatBot and based on the understanding you arrive at in this conversation explain what the following (five cells of) are illustrating"
      ],
      "metadata": {
        "id": "5jPX4d06iiaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
        "\n",
        "# Replace \"NaN\" (in the \"Type 2\" column with \"None\")\n",
        "pokeaman.fillna('None', inplace=True)\n",
        "\n",
        "np.random.seed(130)\n",
        "pokeaman_train,pokeaman_test = \\\n",
        "  train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
        "pokeaman_train\n",
        "model_spec3 = smf.ols(formula='HP ~ Attack + Defense',\n",
        "                      data=pokeaman_train)\n",
        "model3_fit = model_spec3.fit()\n",
        "model3_fit.summary()\n",
        "yhat_model3 = model3_fit.predict(pokeaman_test)\n",
        "y = pokeaman_test.HP\n",
        "print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
        "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model3)[0,1]**2)\n",
        "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'\n",
        "model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
        "\n",
        "\n",
        "model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
        "model4_fit = model4_spec.fit()\n",
        "model4_fit.summary()\n",
        "yhat_model4 = model4_fit.predict(pokeaman_test)\n",
        "y = pokeaman_test.HP\n",
        "print(\"'In sample' R-squared:    \", model4_fit.rsquared)\n",
        "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model4)[0,1]**2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HTwJmysij3x",
        "outputId": "5140d001-4ef2-4532-e6ba-240ef02c6b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'In sample' R-squared:     0.14771558304519905\n",
            "'Out of sample' R-squared: 0.21208501873920738\n",
            "'In sample' R-squared:     0.4670944211413348\n",
            "'Out of sample' R-squared: 0.0024853491144003963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided Python code works with a dataset named pokeaman, which seems to be based on Pokémon data, and applies statistical modeling through linear regression. Here’s a breakdown of the code and the outcomes it illustrates:\n",
        "\n",
        "1. **Data Preparation**:\n",
        "   ```python\n",
        "   import numpy as np\n",
        "   from sklearn.model_selection import train_test_split\n",
        "\n",
        "   fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
        "   pokeaman.fillna('None', inplace=True)\n",
        "   np.random.seed(130)\n",
        "   pokeaman_train, pokeaman_test = train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
        "   ```\n",
        "   - The code begins by importing necessary libraries and setting a random seed for reproducibility.\n",
        "   - It replaces any NaN values in the \"Type 2\" column with \"None\", possibly to handle missing data before modeling.\n",
        "   - The dataset is split into two equal parts for training and testing, ensuring that both subsets are representative of the whole.\n",
        "\n",
        "2. **Model 3: Simple Linear Regression**:\n",
        "   ```python\n",
        "   import statsmodels.formula.api as smf\n",
        "\n",
        "   model_spec3 = smf.ols(formula='HP ~ Attack + Defense', data=pokeaman_train)\n",
        "   model3_fit = model_spec3.fit()\n",
        "   model3_fit.summary()\n",
        "   yhat_model3 = model3_fit.predict(pokeaman_test)\n",
        "   y = pokeaman_test.HP\n",
        "   print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
        "   print(\"'Out of sample' R-squared:\", np.corrcoef(y, yhat_model3)[0,1]**2)\n",
        "   ```\n",
        "   - This model uses ordinary least squares (OLS) regression to predict Pokémon's HP based on their Attack and Defense values.\n",
        "   - The \"in sample\" R-squared value shows how much of the variation in HP is explained by the model within the training set. The \"out of sample\" R-squared, calculated as the square of the correlation between actual and predicted HP in the test set, indicates predictive accuracy on unseen data.\n",
        "\n",
        "3. **Model 4: Complex Linear Regression with Multiple Interactions**:\n",
        "   ```python\n",
        "   model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
        "   # Warning about adding more interactions due to computational limitations\n",
        "   model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
        "   model4_fit = model4_spec.fit()\n",
        "   model4_fit.summary()\n",
        "   yhat_model4 = model4_fit.predict(pokeaman_test)\n",
        "   y = pokeaman_test.HP\n",
        "   print(\"'In sample' R-squared:    \", model4_fit.rsquared)\n",
        "   print(\"'Out of sample' R-squared:\", np.corrcoef(y, yhat_model4)[0,1]**2)\n",
        "   ```\n",
        "   - This model incorporates interactions between multiple variables, including Attack, Defense, Speed, Legendary status, and Special Attack and Defense.\n",
        "   - The formula captures complex relationships, but there’s a caution about the computational load and potential overfitting with too many interactions.\n",
        "\n",
        "\n",
        "**Results**:\n",
        "- Model 3 (\"In sample\" R²: 0.1477, \"Out of sample\" R²: 0.2121) provides moderate predictive power. It performs slightly better on the test data than on the training data, which may suggest robustness or some unexpected fit characteristics.\n",
        "\n",
        "- Model 4 (\"In sample\" R²: 0.4671, \"Out of sample\" R²: 0.0025) demonstrates a classic example of overfitting. Although it explains a large portion of the training data variance, it performs poorly on test data, indicating that it fails to generalize to new data.\n",
        "\n",
        "\n",
        "**Discussion Conclusion**:\n",
        "This progression from a simpler model (Model 3) to a more complex one (Model 4) highlights the trade-off between model complexity and predictive effectiveness, emphasizing the need to balance detail and generalizability in predictive modeling."
      ],
      "metadata": {
        "id": "kLCa1XqQ__89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Work with a ChatBot to understand how the model4_linear_form (linear form specification of model4) creates new predictor variables as the columns of the so-called \"design matrix\" model4_spec.exog (model4_spec.exog.shape) used to predict the outcome variable model4_spec.endog and why the so-called multicollinearity in this \"design matrix\" (observed in np.corrcoef(model4_spec.exog)) contribues to the lack of \"out of sample\" generalization of predictions from model4_fit; then, explain this consisely in your own works\n"
      ],
      "metadata": {
        "id": "56LEKSXznZOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"Cond. No.\" WAS 343.0 WITHOUT to centering and scaling\n",
        "model3_fit.summary()\n",
        "from patsy import center, scale\n",
        "\n",
        "model3_linear_form_center_scale = \\\n",
        "  'HP ~ scale(center(Attack)) + scale(center(Defense))'\n",
        "model_spec3_center_scale = smf.ols(formula=model3_linear_form_center_scale,\n",
        "                                   data=pokeaman_train)\n",
        "model3_center_scale_fit = model_spec3_center_scale.fit()\n",
        "model3_center_scale_fit.summary()\n",
        "# \"Cond. No.\" is NOW 1.66 due to centering and scaling\n",
        "model4_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Defense))'\n",
        "model4_linear_form_CS += ' * scale(center(Speed)) * Legendary'\n",
        "model4_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
        "# Legendary is an indicator, so we don't center and scale that\n",
        "\n",
        "model4_CS_spec = smf.ols(formula=model4_linear_form_CS, data=pokeaman_train)\n",
        "model4_CS_fit = model4_CS_spec.fit()\n",
        "model4_CS_fit.summary().tables[-1]  # Cond. No. is 2,250,000,000,000,000\n",
        "\n",
        "# The condition number is still bad even after centering and scaling\n",
        "# Just as the condition number was very bad to start with\n",
        "model4_fit.summary().tables[-1]  # Cond. No. is 12,000,000,000,000,000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "csSkvn14naJX",
        "outputId": "124e237d-1816-4543-a83c-91652a61da66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.table.SimpleTable'>"
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.671</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.20e+16</td>\n",
              "</tr>\n",
              "</table>"
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.671  \\\\\n\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.20e+16  \\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}"
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When building regression models, particularly those with multiple predictors and interaction terms, it’s essential to understand how these variables contribute to the model's design matrix and how they might impact model stability and performance. This discussion explores issues related to multicollinearity and conditioning in the context of Model 4’s design matrix.\n",
        "\n",
        "### Understanding the Design Matrix (`model4_spec.exog`)\n",
        "\n",
        "In linear regression, the design matrix (often represented as X) organizes the predictor variables in a structured format that supports efficient regression analysis. Each column corresponds to a predictor variable, and each row represents an observation.\n",
        "\n",
        "For Model 4, given by:\n",
        "\n",
        "```python\n",
        "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
        "```\n",
        "\n",
        "This formula includes multiple interaction terms that significantly increase the dimensions of the design matrix. Interaction terms introduce new variables by multiplying the original predictors, allowing the model to capture how predictors influence each other.\n",
        "\n",
        "\n",
        "\n",
        "### Impact of Multicollinearity\n",
        "\n",
        "Multicollinearity arises when one or more predictor variables in a regression model are highly correlated with each other. This correlation complicates identifying the unique impact of each predictor, as changes in one predictor often correspond to changes in others. In the design matrix, multicollinearity can lead to:\n",
        "\n",
        "- Large variances in coefficient estimates, meaning that slight changes in data can cause substantial shifts in model estimates.\n",
        "\n",
        "- Difficulty in inverting the matrix (X'X) during the computation of the regression coefficients, which is essential for ordinary least squares (OLS) regression.\n",
        "\n",
        "### Conditioning and Its Effects\n",
        "\n",
        "The **condition number** of the matrix provides a measure of how sensitive a function is to changes in input or errors, and it applies here to the sensitivity of the linear regression's output to errors in its input data. High condition numbers often indicate multicollinearity and can lead to numerical instability, which might explain the lack of out-of-sample generalization seen in `model4_fit`. High condition numbers indicate that the matrix inversion needed for coefficient calculation is unstable or poorly conditioned.\n",
        "\n",
        "\n",
        "In the code provided, even after centering and scaling the predictors (a technique that can help reduce multicollinearity by putting variables on a similar scale):\n",
        "```python\n",
        "model4_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Defense)) * scale(center(Speed)) * Legendary * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
        "```\n",
        "the condition number is still extremely high (`2,250,000,000,000,000`), indicating significant multicollinearity remains. This is likely due to the extensive interaction terms which include all combinations of multiple scaled and centered variables along with the `Legendary` indicator. This leads to a complex, high-dimensional space where the variables are not only inter-related but also scaled transformations of each other, contributing to the severe multicollinearity.\n",
        "\n",
        "### Consequences for Model Performance\n",
        "\n",
        "The very high condition number and persistent multicollinearity contribute to the model’s poor performance on new data. Although the model may explain the training data well (as suggested by a high in-sample R-squared), it struggles to generalize, leading to a sharp decline in out-of-sample R-squared. This indicates that the model is overfitting to the noise and collinearity within the training data instead of identifying reliable, generalizable patterns.\n",
        "\n",
        "### Summary\n",
        "\n",
        "This analysis highlights the challenges in building effective predictive models when interaction terms and multicollinearity are present. It underscores the importance of selecting predictors carefully, thoughtfully considering interaction terms, and using diagnostics like condition numbers to ensure that models are stable and able to generalize beyond the training data."
      ],
      "metadata": {
        "id": "TTEtkSZZr-xP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's something a little more reasonable...\n",
        "model5_linear_form = 'HP ~ Attack + Defense + Speed + Legendary'\n",
        "model5_linear_form += ' + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
        "model5_linear_form += ' + C(Generation) + C(Q(\"Type 1\")) + C(Q(\"Type 2\"))'\n",
        "\n",
        "model5_spec = smf.ols(formula=model5_linear_form, data=pokeaman_train)\n",
        "model5_fit = model5_spec.fit()\n",
        "model5_fit.summary()\n",
        "yhat_model5 = model5_fit.predict(pokeaman_test)\n",
        "y = pokeaman_test.HP\n",
        "print(\"'In sample' R-squared:    \", model5_fit.rsquared)\n",
        "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model5)[0,1]**2)\n",
        "# Here's something a little more reasonable...\n",
        "model6_linear_form = 'HP ~ Attack + Speed + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
        "# And here we'll add the significant indicators from the previous model\n",
        "# https://chatgpt.com/share/81ab88df-4f07-49f9-a44a-de0cfd89c67c\n",
        "model6_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
        "model6_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
        "model6_linear_form += ' + I(Generation==2)'\n",
        "model6_linear_form += ' + I(Generation==5)'\n",
        "\n",
        "model6_spec = smf.ols(formula=model6_linear_form, data=pokeaman_train)\n",
        "model6_fit = model6_spec.fit()\n",
        "model6_fit.summary()\n",
        "yhat_model6 = model6_fit.predict(pokeaman_test)\n",
        "y = pokeaman_test.HP\n",
        "print(\"'In sample' R-squared:    \", model6_fit.rsquared)\n",
        "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2)\n",
        "# And here's a slight change that seems to perhaps improve prediction...\n",
        "model7_linear_form = 'HP ~ Attack * Speed * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
        "model7_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
        "model7_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
        "model7_linear_form += ' + I(Generation==2)'\n",
        "model7_linear_form += ' + I(Generation==5)'\n",
        "\n",
        "model7_spec = smf.ols(formula=model7_linear_form, data=pokeaman_train)\n",
        "model7_fit = model7_spec.fit()\n",
        "model7_fit.summary()\n",
        "yhat_model7 = model7_fit.predict(pokeaman_test)\n",
        "y = pokeaman_test.HP\n",
        "print(\"'In sample' R-squared:    \", model7_fit.rsquared)\n",
        "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2)\n",
        "# And here's a slight change that seems to perhas improve prediction...\n",
        "model7_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Speed))'\n",
        "model7_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
        "# We DO NOT center and scale indicator variables\n",
        "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
        "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Water\")'\n",
        "model7_linear_form_CS += ' + I(Generation==2)'\n",
        "model7_linear_form_CS += ' + I(Generation==5)'\n",
        "\n",
        "model7_CS_spec = smf.ols(formula=model7_linear_form_CS, data=pokeaman_train)\n",
        "model7_CS_fit = model7_CS_spec.fit()\n",
        "model7_CS_fit.summary().tables[-1]\n",
        "# \"Cond. No.\" is NOW 15.4 due to centering and scaling\n",
        "# \"Cond. No.\" WAS 2,340,000,000 WITHOUT to centering and scaling\n",
        "model7_fit.summary().tables[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "KwcJmONNrnJX",
        "outputId": "72b99227-1cdf-4e83-d31a-54bec23245c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'In sample' R-squared:     0.3920134083531893\n",
            "'Out of sample' R-squared: 0.3001561448865219\n",
            "'In sample' R-squared:     0.3326310334310908\n",
            "'Out of sample' R-squared: 0.2957246042708008\n",
            "'In sample' R-squared:     0.37818209127432456\n",
            "'Out of sample' R-squared: 0.3505538923467793\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.table.SimpleTable'>"
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>2.34e+09</td>\n",
              "</tr>\n",
              "</table>"
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } & 2.34e+09  \\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}"
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transition from **model3_fit** to **model7_fit** demonstrates a strategic process of refining a statistical model to enhance predictive accuracy and generalizability. Below is a breakdown of how each model builds on the previous ones, the reasoning for these additions, and the ways in which they address challenges like multicollinearity and the identification of predictive relationships.\n",
        "\n",
        "### Summary of Model Development:\n",
        "\n",
        "1. **Model3_fit to Model5_fit**:\n",
        "   - **Enhancement**: **Model5_fit** builds on **model3_fit** by adding more predictors (such as Speed, Legendary, Special Defense, Special Attack) and including categorical variables (Generation, Type 1, Type 2).\n",
        "   - **Purpose**: This expansion aims to explain more of the variation in HP by adding features that might impact HP. Including categorical variables helps capture potential group effects, like variations between Pokémon generations or types.\n",
        "   - **Multicollinearity**: Introducing many predictors, especially categorical ones that create additional dummy variables, increases the risk of multicollinearity. However, the model aims to harness the extra predictive power despite this challenge.\n",
        "\n",
        "2. **Model5_fit to Model6_fit**:\n",
        "   - **Enhancement**: **Model6_fit** streamlines the model by removing less significant predictors and focusing on the most predictive categorical variables (specific Types and Generations).\n",
        "   - **Purpose**: This simplification is based on statistical testing, which shows that certain types and generations provide stronger predictive power. The refined model reduces multicollinearity while retaining key explanatory variables.\n",
        "   - **Generalizability**: By concentrating on fewer, more relevant variables, this model may generalize better, minimizing overfitting compared to **model5_fit**.\n",
        "\n",
        "3. **Model6_fit to Model7_fit**:\n",
        "   - **Enhancement**: **Model7_fit** introduces interaction terms among continuous variables (Attack, Speed, Sp. Def, Sp. Atk) and retains important categorical indicators.\n",
        "   - **Purpose**: Adding interaction terms tests whether the influence of one variable on HP depends on another, aiming to capture complex relationships among the attributes.\n",
        "   - **Addressing Multicollinearity**: While interactions can increase multicollinearity, the choice of interactions is guided by their potential relevance to HP, balancing added complexity with improved predictive capability.\n",
        "\n",
        "### Handling Multicollinearity:\n",
        "The progression in these models is accompanied by attention to multicollinearity, as seen in the \"condition numbers\" (indicators of multicollinearity). While more complex models tend to have higher condition numbers, **model7_fit** uses centering and scaling to reduce multicollinearity, resulting in a condition number of 15.4, which is within an acceptable range, compared to much higher numbers in untransformed models.\n",
        "\n",
        "### Final Thoughts:\n",
        "The model development process seeks to balance predictive strength with avoiding overfitting. Moving from simpler to more intricate models represents a deliberate effort to add predictive features, refine the model based on statistical results, and mitigate numerical issues through variable transformation. As the models evolve, they improve from potentially underfitting the data to more accurately capturing the data's complexity. Out-of-sample R-squared values indicate steady improvements, showing that each iteration offers better generalization, culminating in **model7**’s enhanced performance and stability."
      ],
      "metadata": {
        "id": "BW6UUWfnr8WL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Work with a ChatBot to write a for loop to create, collect, and visualize many different paired \"in sample\" and \"out of sample\" model performance metric actualizations (by not using np.random.seed(130) within each loop iteration); and explain in your own words the meaning of your results and purpose of this demonstration"
      ],
      "metadata": {
        "id": "rlZyMxaytDh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To address the query about demonstrating the effect of random training test splits on model performance, we will construct a loop in Python that creates multiple realizations of “in-sample” and “out-of-sample” r-squared values for the linear regression model. This will help illustrate the variability in model performance due to different random splits of the data, without the need to fix the random seed. This variability can provide insight into the stability and generalization of the model.\n",
        "\n",
        "### Objective\n",
        "The goal of this demonstration is to assess how sensitive the model's performance is to different randomly selected subsets of data. This helps identify whether the model may be overfitting to the training data and provides insight into how well it might perform on new, unseen data.\n",
        "\n",
        "### Explanation of Results and Purpose\n",
        "- **R-squared Variability**: This visualization displays the \"in-sample\" versus \"out-of-sample\" R-squared values for each iteration. Points closer to the diagonal line (y=x) indicate that the model performs similarly on both the training and testing sets, suggesting good generalization.\n",
        "- **Interpretation**:\n",
        "  - **Points above the line**: Show cases where the model performs better on the test set than on the training set. This can happen when a particular data split aligns well with the model's structure, or when the training set has more noise.\n",
        "  - **Points below the line**: Indicate overfitting, where the model closely matches the training data's specifics but struggles to generalize to new data.\n",
        "\n",
        "- **General Observations**: If many points fall significantly below the line across various random splits, it suggests consistent overfitting. On the other hand, points near or above the line imply a robust model that generalizes well.\n",
        "\n",
        "This experiment underscores the importance of cross-validation and thorough model evaluation to avoid misleading results from a single data split. By examining multiple splits and the distribution of results, we gain a clearer understanding of the model’s reliability and potential real-world performance."
      ],
      "metadata": {
        "id": "CqoAMqlJswBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.model_selection import train_test_split\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# Assuming 'pokeaman' DataFrame is preloaded\n",
        "reps = 100\n",
        "in_sample_Rsquared = np.zeros(reps)\n",
        "out_of_sample_Rsquared = np.zeros(reps)\n",
        "\n",
        "for i in range(reps):\n",
        "    pokeaman_train, pokeaman_test = train_test_split(pokeaman, train_size=0.5)\n",
        "    model_spec = smf.ols(formula='HP ~ Attack + Defense', data=pokeaman_train)\n",
        "    model_fit = model_spec.fit()\n",
        "\n",
        "    # In-sample R-squared\n",
        "    in_sample_Rsquared[i] = model_fit.rsquared\n",
        "\n",
        "    # Out-of-sample R-squared\n",
        "    predictions = model_fit.predict(pokeaman_test)\n",
        "    out_of_sample_Rsquared[i] = np.corrcoef(pokeaman_test['HP'], predictions)[0, 1] ** 2\n",
        "\n",
        "# Creating a DataFrame to hold results\n",
        "results_df = pd.DataFrame({\n",
        "    \"In Sample R-squared\": in_sample_Rsquared,\n",
        "    \"Out of Sample R-squared\": out_of_sample_Rsquared\n",
        "})\n",
        "\n",
        "# Plotting\n",
        "fig = px.scatter(results_df, x=\"In Sample R-squared\", y=\"Out of Sample R-squared\", title=\"Model Performance Variation\")\n",
        "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], name=\"y=x\", mode='lines', line_shape='linear', line_dash='dash'))\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "xgpPNYxrs0fP",
        "outputId": "d8f8820a-e975-4469-ed68-35ddc200d427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"1455c1d4-a6dd-4b5e-b620-4e6e301aa9b8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1455c1d4-a6dd-4b5e-b620-4e6e301aa9b8\")) {                    Plotly.newPlot(                        \"1455c1d4-a6dd-4b5e-b620-4e6e301aa9b8\",                        [{\"hovertemplate\":\"In Sample R-squared=%{x}\\u003cbr\\u003eOut of Sample R-squared=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0.1678367784811714,0.1106338754112931,0.2180252435846014,0.2247685059961666,0.1930648587344811,0.08736776568684879,0.23288670034407188,0.11838142587409073,0.21932073090607662,0.23395968115467036,0.1837227507596677,0.31027605307729733,0.17446272568439947,0.19454495605237732,0.20091261610892508,0.26171058427318694,0.21353066204339555,0.15303386396357765,0.2252228876996677,0.15264261160233272,0.21455739745509583,0.3131236042002644,0.11163091513197654,0.1954061237698429,0.11945135996017253,0.2286780432439801,0.17323123371227722,0.1058249717287657,0.16885281804993102,0.16181430196730628,0.1953575872715917,0.17491554267906861,0.10546525630817372,0.2919347991723634,0.19898400701944596,0.19317604994505932,0.12150909379820063,0.17241361286553225,0.10024983253580644,0.20337755848793437,0.18068037141632498,0.2703559187986563,0.1705087365523824,0.17148768769584444,0.16655780525172037,0.22634742861243717,0.21136421183179366,0.20670943192350377,0.1871671832109303,0.26324712646007764,0.18320225374344212,0.23694459428466652,0.25630615717920613,0.2716887637087,0.18205482280932384,0.141759538846153,0.24198061060384424,0.17648764597980138,0.14966848288776502,0.2006304701237559,0.22886425932250987,0.15381251264049955,0.2538913451064774,0.11792735972745061,0.22429211012039985,0.2939251438246523,0.13060158478822037,0.22798833417562592,0.2447200668299837,0.19773583190707644,0.2673253617485081,0.2536299694216918,0.13752754334777695,0.15244343834795193,0.16138591281406034,0.33034811090554206,0.15213717679755445,0.15922989155807588,0.19670474961432105,0.10133192755710929,0.21810731762934843,0.1917559871631851,0.18744720091102907,0.25685201069064867,0.1805763884275714,0.2654183922468212,0.22547996051019703,0.15263476564557354,0.2782700630283059,0.1850585164778754,0.2838192390157759,0.18707008077761345,0.27368626393049433,0.2589550969743548,0.22696009842826081,0.14093145549821828,0.2076279120058292,0.17647168814257908,0.1536885621950781,0.19845606619460632],\"xaxis\":\"x\",\"y\":[0.19481970763399178,0.2883478435076267,0.1490203667260121,0.1430794278146116,0.1712811444035999,0.34018367409974326,0.14055728978296694,0.2913106904229935,0.15071971059355102,0.1399385477314447,0.17981783523676234,0.10617286974105981,0.18273674831304781,0.16777865085331528,0.16202566977690663,0.12450789434483327,0.15075088470100168,0.21254639647753645,0.14597816375527758,0.21156860407399872,0.15570405101088988,0.09151046146169467,0.295762848462822,0.16979887013345624,0.2546563607405623,0.1423587406180613,0.17867403947628702,0.289085702365421,0.19628123415138615,0.20038624028026003,0.16745214380032328,0.19050656937144048,0.2867474532764758,0.12158084790798165,0.15569845177543745,0.1695247133056129,0.26318720690745506,0.19225302864741842,0.32167588945876563,0.1616687510875496,0.18332481175245652,0.11891402573509359,0.19651974623394336,0.19016608698078666,0.19822264618370442,0.14292479667023117,0.1558828444492605,0.15480440831845868,0.16386533249787952,0.11226068097520847,0.1815380828501591,0.13331893164811995,0.12706400819819055,0.12140846017365449,0.18254047003955842,0.2318454604511801,0.13460381784706654,0.19053968212854375,0.22470209692537635,0.1577326423631331,0.14568919154868717,0.20304011854389623,0.12360405544137498,0.27859884563767306,0.13990675590000015,0.11251555799968249,0.2554881684115262,0.14867564858245805,0.13382468968090727,0.16506460907060225,0.11593590676729615,0.12793566224337305,0.23685207999416558,0.21218820306534392,0.19885526763604264,0.09536348289896675,0.2231686750492331,0.20611529913667495,0.16486321952751865,0.29286843046735384,0.15218662808080455,0.1735250509947412,0.17398616075380785,0.1284205744592806,0.1856242821515053,0.1256684962181535,0.14727137512592564,0.21548027653734345,0.1149858113224365,0.1774374725821576,0.11449679275374082,0.1753865414660696,0.10941778513246678,0.12674824954994782,0.14934070262385601,0.2276119962703254,0.16268136063116945,0.18935316310453706,0.21326138740887743,0.16875279206263555],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"line\":{\"dash\":\"dash\",\"shape\":\"linear\"},\"mode\":\"lines\",\"name\":\"y=x\",\"x\":[0,1],\"y\":[0,1],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"In Sample R-squared\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Out of Sample R-squared\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Model Performance Variation\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1455c1d4-a6dd-4b5e-b620-4e6e301aa9b8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Work with a ChatBot to understand the meaning of the illustration below; and, explain this in your own word"
      ],
      "metadata": {
        "id": "OGaBdVxUteSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model7_gen1_predict_future = smf.ols(formula=model7_linear_form,\n",
        "                                   data=pokeaman[pokeaman.Generation==1])\n",
        "model7_gen1_predict_future_fit = model7_gen1_predict_future.fit()\n",
        "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
        "y = pokeaman_test.HP\n",
        "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
        "print(\"'In sample' R-squared:    \", model7_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
        "y = pokeaman[pokeaman.Generation!=1].HP\n",
        "yhat = model7_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
        "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")\n",
        "model7_gen1to5_predict_future = smf.ols(formula=model7_linear_form,\n",
        "                                   data=pokeaman[pokeaman.Generation!=6])\n",
        "model7_gen1to5_predict_future_fit = model7_gen1to5_predict_future.fit()\n",
        "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
        "y = pokeaman_test.HP\n",
        "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
        "print(\"'In sample' R-squared:    \", model7_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
        "y = pokeaman[pokeaman.Generation==6].HP\n",
        "yhat = model7_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
        "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")\n",
        "model6_gen1_predict_future = smf.ols(formula=model6_linear_form,\n",
        "                                   data=pokeaman[pokeaman.Generation==1])\n",
        "model6_gen1_predict_future_fit = model6_gen1_predict_future.fit()\n",
        "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
        "y = pokeaman_test.HP\n",
        "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
        "print(\"'In sample' R-squared:    \", model6_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
        "y = pokeaman[pokeaman.Generation!=1].HP\n",
        "yhat = model6_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
        "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")\n",
        "model6_gen1to5_predict_future = smf.ols(formula=model6_linear_form,\n",
        "                                   data=pokeaman[pokeaman.Generation!=6])\n",
        "model6_gen1to5_predict_future_fit = model6_gen1to5_predict_future.fit()\n",
        "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
        "y = pokeaman_test.HP\n",
        "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
        "print(\"'In sample' R-squared:    \", model6_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
        "y = pokeaman[pokeaman.Generation==6].HP\n",
        "yhat = model6_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
        "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_XHvLq_tTK4",
        "outputId": "fde91172-f339-4375-b12d-6b44ec62ea10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'In sample' R-squared:     0.37818209127432456 (original)\n",
            "'Out of sample' R-squared: 0.00018905947562235405 (original)\n",
            "'In sample' R-squared:     0.5726118179916574 (gen1_predict_future)\n",
            "'Out of sample' R-squared: 0.11151363388299076 (gen1_predict_future)\n",
            "'In sample' R-squared:     0.37818209127432456 (original)\n",
            "'Out of sample' R-squared: 0.00018905947562235405 (original)\n",
            "'In sample' R-squared:     0.3904756578094535 (gen1to5_predict_future)\n",
            "'Out of sample' R-squared: 0.2339491543502001 (gen1to5_predict_future)\n",
            "'In sample' R-squared:     0.3326310334310908 (original)\n",
            "'Out of sample' R-squared: 0.00039016276723082656 (original)\n",
            "'In sample' R-squared:     0.44338805177272833 (gen1_predict_future)\n",
            "'Out of sample' R-squared: 0.19328585342762092 (gen1_predict_future)\n",
            "'In sample' R-squared:     0.3326310334310908 (original)\n",
            "'Out of sample' R-squared: 0.00039016276723082656 (original)\n",
            "'In sample' R-squared:     0.335172798241148 (gen1to5_predict_future)\n",
            "'Out of sample' R-squared: 0.2626269017880005 (gen1to5_predict_future)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The discussion you shared explains a method for assessing how well different models predict future outcomes using historical data, divided by a time-based variable like \"Generation\" in the Pokémon dataset. This evaluation approach highlights the need for models to generalize well, especially when newer data may exhibit different trends than past data. Let’s break down the key ideas and implications of this approach.\n",
        "\n",
        "### Sequential Data Approach Explained\n",
        "\n",
        "The code applies a real-world scenario where historical data is used to predict future events. Here’s an overview of the process:\n",
        "\n",
        "1. **Training Models on Older Data**: Models are trained on data from earlier generations (e.g., only Generation 1 data or data from Generations 1-5).\n",
        "2. **Testing on Newer Data**: These trained models are then used to make predictions for Pokémon from newer generations (e.g., starting from Generation 2 or exclusively Generation 6), which were not included in the training set.\n",
        "\n",
        "This process checks whether a model trained on past data can accurately predict results on new or unseen data. This is essential in real-world applications where models need to work well on data that might differ from the original training data.\n",
        "\n",
        "### Analyzing the Results\n",
        "\n",
        "The code compares \"in-sample\" R-squared (model performance on training data) and \"out-of-sample\" R-squared (model performance on testing data) for both models (Model 6 and Model 7). This sequential approach reveals:\n",
        "\n",
        "- **Complexity vs. Generalizability**: Although Model 7 may perform better on a mixed train-test set, it might not predict future generations as reliably as Model 6. This could be due to Model 7's higher complexity, which might capture specific patterns in the training data that don’t generalize well (\"overfitting\").\n",
        "- **Favoring a Simpler Model**: Model 6, being simpler, may not achieve as high \"in-sample\" R-squared as Model 7 but exhibits more consistent \"out-of-sample\" performance, indicating better generalizability for new generations.\n",
        "\n",
        "### Key Points for Model Selection\n",
        "\n",
        "- **Simplicity and Interpretability**: Simpler models, like Model 6, are easier to interpret and may still provide adequate predictive power with better resilience to shifts in data patterns. This is crucial in fields where understanding model decisions is as important as prediction accuracy.\n",
        "- **Forward-Looking Predictions**: When predicting future outcomes, a balance between complexity and simplicity is essential. Highly complex models might perform well on past data but struggle to adapt to new data trends.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "This example using sequential generation-based data splits emphasizes the importance of model simplicity and adaptability to new data. It shows that while complex models can excel on known data, simpler models often deliver more reliable predictions on future or evolving data. The main takeaway is that the best model is not necessarily the one with the highest accuracy on historical data, but rather the one that consistently provides reliable and interpretable predictions across various datasets."
      ],
      "metadata": {
        "id": "X7uyeEZFvV8f"
      }
    }
  ]
}